import hashlib
import logging
import os
import functions_framework

from google.api_core.exceptions import GoogleAPICallError, NotFound
from google.cloud import discoveryengine_v1 as discoveryengine
from google.cloud import logging as cloud_logging

# --- Configuration ---
PROJECT_ID = os.environ.get("PROJECT_ID")
LOCATION = os.environ.get("LOCATION")
DATA_STORE_ID = os.environ.get("DATA_STORE_ID")

# --- Logging Setup ---
# Instantiate a client for Google Cloud Logging.
logging_client = cloud_logging.Client()
# Attaches a Google Cloud Logging handler to the root logger.
logging_client.setup_logging()

# Get a logger for this module.
log = logging.getLogger(__name__)

# --- Constants ---
ALLOWED_EXTENSIONS = {".html", ".pdf", ".docx", ".pptx", ".txt", ".xlsx"}
GCS_OBJECT_FINALIZED = "google.cloud.storage.object.v1.finalized"
GCS_OBJECT_DELETED = "google.cloud.storage.object.v1.deleted"

# Initialize the Vertex AI Search Document Service client
document_service_client = discoveryengine.DocumentServiceClient()


def get_vertex_autogenerated_id(gcs_uri: str) -> str:
    """Replicates Vertex AI Search's automatic ID generation."""
    sha256_hash = hashlib.sha256(gcs_uri.encode("utf-8")).hexdigest()
    return sha256_hash[:32]


def handle_upsert(bucket_name: str, file_name: str):
    """Handles file creation/update using the efficient gcs_source method."""
    log.info(f"Handling UPSERT for: gs://{bucket_name}/{file_name}")

    _, extension = os.path.splitext(file_name)
    if extension.lower() not in ALLOWED_EXTENSIONS:
        log.info(
            f"Skipping file '{file_name}' with unsupported extension '{extension}'."
        )
        return

    gcs_uri = f"gs://{bucket_name}/{file_name}"
    parent = document_service_client.branch_path(
        project=PROJECT_ID,
        location=LOCATION,
        data_store=DATA_STORE_ID,
        branch="default_branch",
    )
    request = discoveryengine.ImportDocumentsRequest(
        parent=parent,
        gcs_source=discoveryengine.GcsSource(
            input_uris=[gcs_uri], data_schema="content"
        ),
        reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,
    )

    try:
        operation = document_service_client.import_documents(request=request)
        log.info(
            f"Started Vertex AI Search import operation: {operation.operation.name}"
        )
        predicted_id = get_vertex_autogenerated_id(gcs_uri)
        log.info(
            f"File '{gcs_uri}' will be indexed with predicted Document ID '{predicted_id}'."
        )
    except GoogleAPICallError as e:
        log.error(
            f"Error calling Vertex AI Search API for file {gcs_uri}: {e}", exc_info=True
        )
        raise


def handle_delete(bucket_name: str, file_name: str):
    """Handles file deletion by calculating the autogenerated document ID."""
    log.info(f"Handling DELETE for: gs://{bucket_name}/{file_name}")

    gcs_uri = f"gs://{bucket_name}/{file_name}"
    document_id = get_vertex_autogenerated_id(gcs_uri)
    document_name = document_service_client.document_path(
        project=PROJECT_ID,
        location=LOCATION,
        data_store=DATA_STORE_ID,
        branch="default_branch",
        document=document_id,
    )

    try:
        log.info(f"Attempting to delete Document ID '{document_id}'...")
        document_service_client.delete_document(name=document_name)
        log.info(f"Successfully deleted document '{document_name}'.")
    except NotFound:
        log.info(f"Document '{document_name}' not found. Nothing to delete.")
    except GoogleAPICallError as e:
        log.error(f"Error deleting document '{document_name}': {e}", exc_info=True)
        raise


@functions_framework.cloud_event
def sync_vertex_search(cloud_event):
    """Cloud Function entry point that dispatches based on the GCS event type."""
    event_type = cloud_event["type"]
    data = cloud_event.data

    if not all([data, PROJECT_ID, LOCATION, DATA_STORE_ID]):
        log.error("FATAL: Missing event data or required environment variables.")
        return

    bucket_name = data.get("bucket")
    file_name = data.get("name")

    if not all([bucket_name, file_name]):
        log.error("FATAL: Event data is missing 'bucket' or 'name' field.")
        return

    if event_type == GCS_OBJECT_FINALIZED:
        handle_upsert(bucket_name, file_name)
    elif event_type == GCS_OBJECT_DELETED:
        handle_delete(bucket_name, file_name)
    else:
        log.warning(f"Received unhandled event type '{event_type}'. Ignoring.")
